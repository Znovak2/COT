{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Assignment\n",
    "*Name:* Zach Novak\n",
    "\n",
    "*PID:* za659148\n",
    "\n",
    "*Date:* 3/9/2025\n",
    "\n",
    "The dataset 'bank_marketing.csv' is related with direct marketing campaigns (phone calls) of a banking institution. The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n",
    "\n",
    "Important: When you read the csv file, you might need to manipulate the columsn somehow for better printing.\n",
    "\n",
    "For more information abour the dataset, please check out the link below:\n",
    "https://archive.ics.uci.edu/dataset/222/bank+marketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: import necessary libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV with the ; separator\n",
    "df = pd.read_csv('data/bank_marketing.csv', sep=';')\n",
    "print(df.head(1))\n",
    "print(df.shape)\n",
    "print(df.info())\n",
    "\n",
    "# check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# summary statistics for numerical columns\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above code that there are no missing values in the dataset.\n",
    "\n",
    "Let's get an inventory of all the unqiue values in each feature so we can plan out the preprocessing approach. This also can be a method to spot bad data or slight variations within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to check for unique values\n",
    "columns_to_check = df.columns\n",
    "\n",
    "# print unique values for each column\n",
    "for column in columns_to_check:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in '{column}':\\n{unique_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of features:\n",
    "\n",
    "- age: \n",
    "    - age of the client\n",
    "- job: \n",
    "    - the job of the client\n",
    "- marital: \n",
    "    - marital status of the client\n",
    "- education: \n",
    "    - education level of the client\n",
    "- default: \n",
    "    - whether the client has credit in default\n",
    "- housing: \n",
    "    - whether the client has a housing loan\n",
    "- loan: \n",
    "    - whether the client has a personal loan\n",
    "- contact: \n",
    "    - type of contact method (cellular or telephone)\n",
    "- month: \n",
    "    - month of the last contact\n",
    "- day_of_week: \n",
    "    - day of the week of the last contact\n",
    "- duration: \n",
    "    - duration of the last contact in seconds\n",
    "- campaign: \n",
    "    - number of contacts during the current campaign\n",
    "- pdays: \n",
    "    - number of days since the client was last contacted in a previous campaign (999 indicates no previous contact)\n",
    "- previous: \n",
    "    - number of contacts performed before the current campaign\n",
    "- poutcome: \n",
    "    - outcome of the previous marketing campaign (nonexistent, failure, success)\n",
    "- emp.var.rate: \n",
    "    - employment variation rate (economic indicator)\n",
    "- cons.price.idx: \n",
    "    - consumer price index (economic indicator)\n",
    "- cons.conf.idx: \n",
    "    - consumer confidence index (economic indicator)\n",
    "- euribor3m: \n",
    "    - 3-month Euribor rate (economic indicator)\n",
    "- nr.employed: \n",
    "    - number of employees (economic indicator)\n",
    "- y: \n",
    "    - whether the client subscribed to a term deposit (target variable)\n",
    "\n",
    "To formulate my preprocessing approach, first, I categorize each feature into one of the two broad types: Numeric and Categorical.\n",
    "\n",
    "Numerical features in the dataset are:\n",
    "- age\n",
    "- duration\n",
    "- campaign\n",
    "- pdays\n",
    "- previous\n",
    "- emp.var.rate\n",
    "- cons.price.idx\n",
    "- cons.conf.idx\n",
    "- euribor3m\n",
    "- nr.employed\n",
    "\n",
    "Categorical categories are:\n",
    "- job\n",
    "- marital\n",
    "- education\n",
    "- default\n",
    "- housing\n",
    "- loan\n",
    "- contact\n",
    "- month\n",
    "- day_of_week\n",
    "- poutcome\n",
    "- y\n",
    "\n",
    "We can:\n",
    "- Remove duplicate entries from the dataset\n",
    "- Encode variables to a more machine readable format.\n",
    "- Replace '999' in the pdays feature by creating a new feature called \"first_campaign\" since the '999' value means the client wasn't contacted before.with 'unknown' since it seems to be a placeholder\n",
    "- We have already validated the data integrity such as typos or unreasonable values like age > 100 for the features.\n",
    "- We have already validated there are not any NULL values.\n",
    "\n",
    "Later in the notebook, you will see resampling done to the dataset to address a sample imbalance that was identified during the initial training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "print(\"\\nNumber of duplicates in the initial datset:\", df.duplicated().sum())\n",
    "\n",
    "# dop dupicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"\\nNumber of duplicates after dropping:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Features\n",
    "\n",
    "Most of these features will be scaled shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the 'first_campaign' feature\n",
    "def first_campaign(pdays):\n",
    "    if pdays == 999:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['first_campaign'] = df['pdays'].apply(first_campaign)\n",
    "\n",
    "# display the first few rows to verify the new feature\n",
    "print(df[['pdays', 'first_campaign']].head(10))\n",
    "\n",
    "# print unique values and statistics for the new feature\n",
    "print(\"\\nUnique values in 'first_campaign':\", df['first_campaign'].unique())\n",
    "print(\"\\nCount of first campaign vs returning customers:\")\n",
    "print(df['first_campaign'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "These can be further disected into binary (two values) and categorical features (over two values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the dataset before encoding\n",
    "print(\"Shape of the dataset before encoding:\", df.shape)\n",
    "\n",
    "# first use Label Encoder for binary categorical variables\n",
    "binary_features = ['default', 'housing', 'loan', 'y']\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in binary_features:\n",
    "    df[feature] = label_encoder.fit_transform(df[feature])\n",
    "\n",
    "# use one-hot encoding for non-binary categorical variables\n",
    "categorical_features = ['job', 'contact', 'marital', 'education', 'month', 'day_of_week', 'poutcome']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "# print the shape of the new dataframe to see how many features we now have\n",
    "print(\"Shape of the dataset after encoding:\", df_encoded.shape)\n",
    "print(\"\\nNew features:\", df_encoded.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age, duration, campaign, pdays, cons.price.idx, cons.conf.idx, euribor3m, nr.employed, emp. var.rate are all quantitative features. However, pdays has a special indicator of 999 which would be treated as an outlier incorrectly. The economic indicator features have very narrow ranges and although some extreme ranges may be outliers, they reflect an important time in economic events which can impact sales.\n",
    "Let's review the appropraite features to see if there are any outliers that could skew the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Initial check of feature distribution\n",
    "\n",
    "# list of quantitative features to check for outliers\n",
    "quantitative_features = ['age','duration', 'campaign']\n",
    "\n",
    "# create box plots and histograms for each quantitative feature\n",
    "fig, axes = plt.subplots(len(quantitative_features), 2, figsize=(15, 4*len(quantitative_features)))\n",
    "fig.suptitle('Distribution and Outliers of Quantitative Features', fontsize=16, y=1.02)\n",
    "\n",
    "for i, feature in enumerate(quantitative_features):\n",
    "    # box plot\n",
    "    sns.boxplot(x=df[feature], ax=axes[i,0])\n",
    "    axes[i,0].set_title(f'Box plot of {feature}')\n",
    "    axes[i,0].set_xlabel('Value')\n",
    "    \n",
    "    # histogram with KDE\n",
    "    sns.histplot(df[feature], kde=True, ax=axes[i,1])\n",
    "    axes[i,1].set_title(f'Distribution of {feature}')\n",
    "    axes[i,1].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualiazations which help us understand the data better, we see there are outliers. It is beneficial to filter outliers because of the KNN model's distance based algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Remove outliers using IQR\n",
    "\n",
    "# print the shape of the dataset before removing outliers\n",
    "print(\"Shape of the dataset before removing outliers: \", df.shape)\n",
    "\n",
    "# function to remove outliers using IQR\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# remove outliers for each quantitative feature\n",
    "for feature in quantitative_features:\n",
    "    df = remove_outliers(df, feature)\n",
    "\n",
    "# Print the shape of the dataset after removing outliers\n",
    "print(\"Shape of the dataset after removing outliers: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Check for outliers again\n",
    "\n",
    "# create box plots and histograms for each quantitative feature\n",
    "fig, axes = plt.subplots(len(quantitative_features), 2, figsize=(15, 4*len(quantitative_features)))\n",
    "fig.suptitle('Distribution and Outliers of Quantitative Features', fontsize=16, y=1.02)\n",
    "\n",
    "for i, feature in enumerate(quantitative_features):\n",
    "    # box plot\n",
    "    sns.boxplot(x=df[feature], ax=axes[i,0])\n",
    "    axes[i,0].set_title(f'Box plot of {feature}')\n",
    "    axes[i,0].set_xlabel('Value')\n",
    "    \n",
    "    # histogram with KDE\n",
    "    sns.histplot(df[feature], kde=True, ax=axes[i,1])\n",
    "    axes[i,1].set_title(f'Distribution of {feature}')\n",
    "    axes[i,1].set_xlabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the now encoded dataset\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scale the features (important for KNN)\n",
    "\n",
    "Scaling is very importing in a KNN model since a KNN model is a distance based algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the numerical variables in the Duration and Campaign features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numerical_features = ['age','duration','campaign','previous','emp.var.rate','cons.price.idx']\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# verify the scaling by checking mean (should be ~0) and std (should be ~1)\n",
    "print(\"Scaled features statistics:\")\n",
    "print(df[numerical_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoding increases dimensionality which can impact the KNN model's performance. Because of this, a dimenstionality reduction technique like PCA can help retain the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the dataset before PCA\n",
    "print(\"Shape of the dataset before PCA:\", df_encoded.shape)\n",
    "\n",
    "# define the number of components to keep\n",
    "n_components = 10\n",
    "\n",
    "# initialize PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# separate features and target variable\n",
    "X = df_encoded.drop(columns=['y'])\n",
    "y = df_encoded['y']\n",
    "\n",
    "# fit and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# print the explained variance ratio to see how much variance is captured by each component\n",
    "print(\"Explained variance ratio by each component:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# print the shape of the transformed data\n",
    "print(\"Shape of the data after PCA:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define features (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming df_encoded is your fully preprocessed dataframe\n",
    "y = df_encoded['y']\n",
    "X = df_encoded.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split into training and test sets (80 & 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 80/20 train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6, 7 & 8: Find the optimal K using 5-fold cross-validation, plot the error rates & print the optimal K-value.\n",
    "\n",
    "A small K can be noisy and sensitive to outliers, while a large K may smooth out patterns too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of k values to test\n",
    "k_values = range(1, 41)  # range to test for k\n",
    "error_rates = []\n",
    "\n",
    "# use 5-fold cross-validation to compute the error rate for each k\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # compute cross-validated accuracy on the training data\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    error_rate = 1 - scores.mean()  # error rate = 1 - average accuracy\n",
    "    error_rates.append(error_rate)\n",
    "    print(f\"k = {k}, Error Rate = {error_rate:.4f}\")\n",
    "\n",
    "# identify the optimal k\n",
    "optimal_k = k_values[np.argmin(error_rates)]\n",
    "print(\"\\nOptimal K (with lowest error rate):\", optimal_k)\n",
    "\n",
    "# visualization of error rates vs. k values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, error_rates, marker='o', linestyle='--', color='blue')\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.axvline(x=optimal_k, linestyle='--', color='red', label=f'Optimal K = {optimal_k}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 & 10: Train the final KNN model with the optimal K and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the final KNN model with the optimal K\n",
    "knn_final = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_final.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = knn_final.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Compute the confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Set the tick marks and labels; adjust class labels if necessary\n",
    "classes = ['No', 'Yes']  # Change these if your labels differ\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "This model predicts the majority class (class 0) very well which the high accuracy of the model demonstrates.\n",
    "\n",
    "When it comes to interpreting the minority class (class 1) performance, the recall of 47% demontrates how ineffective the model is at intrepreting the minority class (a term deposit subscriber). This recall number means the model only captures 47% of the actual positive cases meaning more than half of the term deposit subscribers (i.e., opportunities) are being missed. This is further exasturbated by the low F1 score (55%).\n",
    "\n",
    "In additional experimentation, the data sampling will be reviewed for a more optimal handling of the minority class, or in otherwords, accurately handling the business use case needs of finding sales opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Additional experimentation through resampling\n",
    "\n",
    "* Requires an additional package dependency: $pip install imblearn\n",
    "\n",
    "Oversampling with SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# apply SMOTE only on the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# check the new class distribution\n",
    "print(\"Before resampling:\\n\", y_train.value_counts())\n",
    "print(\"\\nAfter resampling:\\n\", pd.Series(y_train_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalculate the optimal K-value using the same methodology as previously in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of k values to test on the resampled data\n",
    "k_values_resampled = range(1, 41) # range to test for k\n",
    "error_rates_resampled = []\n",
    "\n",
    "# use 5-fold cross-validation on the resampled training data to compute error rates\n",
    "for k in k_values_resampled:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "    error_rate = 1 - scores.mean()\n",
    "    error_rates_resampled.append(error_rate)\n",
    "    print(f\"k = {k}, Error Rate = {error_rate:.4f}\")\n",
    "\n",
    "# identify the optimal k value based on the resampled data\n",
    "optimal_k_resampled = k_values_resampled[np.argmin(error_rates_resampled)]\n",
    "print(\"\\nOptimal K on resampled data:\", optimal_k_resampled)\n",
    "\n",
    "# visualize the error rates vs. k values for the resampled data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values_resampled, error_rates_resampled, marker='o', linestyle='--', color='blue')\n",
    "plt.title('Error Rate vs. K Value (Resampled Data)')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.axvline(x=optimal_k_resampled, linestyle='--', color='red', label=f'Optimal K = {optimal_k_resampled}')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the KNN model on Resampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a new KNN model using the optimal_k determined previously, but with resampled data\n",
    "knn_final_resampled = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_final_resampled.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# make predictions on the original test set\n",
    "y_pred_resampled = knn_final_resampled.predict(X_test)\n",
    "\n",
    "print(y_pred_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the new model's performance\n",
    "accuracy_resampled = accuracy_score(y_test, y_pred_resampled)\n",
    "print(\"Test Accuracy after resampling:\", accuracy_resampled)\n",
    "\n",
    "# show the confusion matrix and classification report\n",
    "cm_resampled = confusion_matrix(y_test, y_pred_resampled)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_resampled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_resampled))\n",
    "\n",
    "# visualize the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(cm_resampled, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix (After Resampling)')\n",
    "plt.colorbar()\n",
    "\n",
    "classes = ['No', 'Yes']\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "The total recall for class 1 (potential term deposit subscribers) increased dramatically to 88%. This translates to an 88% prediction rate of actual positives.\n",
    "The resampled data enhances the model's robustness when dealing with the minority class.\n",
    "\n",
    "The model's accuracy dropped to 84% which can happen when addressing class imbalances because the model now focuses more on detecting positives, even if that means sacrificing some accuracy on the majority class.\n",
    "\n",
    "The reducred precision for Class 1 (41%) means that while the model is catching more positive cases, more of them are false positives. This is acceptable for the business use case because the false positives can be screened by a person and it leaves more confidence that the process is not missing opportunities. However, it is important to cost in mind for these screenings as it could potentially not be worth it if there are too many false positives.\n",
    "\n",
    "Overall, these results suggest the model is now more effective at identifying the minority class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
